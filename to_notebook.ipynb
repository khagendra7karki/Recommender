{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with '/bin/python3' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: '/bin/python3 -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import configparser\n",
    "import scipy.sparse as sp\n",
    "import torch.utils.data as data\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import time\n",
    "from torch.utils.tensorboard import SummaryWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def load_data(train_file, test_file):\n",
    "    trainUniqueUsers, trainItem, trainUser = [], [], []\n",
    "    testUniqueUsers, testItem, testUser = [], [], []\n",
    "    n_user, m_item = 0, 0\n",
    "    trainDataSize, testDataSize = 0, 0\n",
    "    with open(train_file, 'r') as f:\n",
    "        for l in f.readlines():\n",
    "            if len(l) > 0:\n",
    "                l = l.strip('\\n').split(' ')\n",
    "                items = [int(i) for i in l[1:]]\n",
    "                uid = int(l[0])\n",
    "                trainUniqueUsers.append(uid)\n",
    "                trainUser.extend([uid] * len(items))\n",
    "                trainItem.extend(items)\n",
    "                m_item = max(m_item, max(items))\n",
    "                n_user = max(n_user, uid)\n",
    "                trainDataSize += len(items)\n",
    "    trainUniqueUsers = np.array(trainUniqueUsers)\n",
    "    trainUser = np.array(trainUser)\n",
    "    trainItem = np.array(trainItem)\n",
    "\n",
    "    with open(test_file) as f:\n",
    "        for l in f.readlines():\n",
    "            if len(l) > 0:\n",
    "                l = l.strip('\\n').split(' ')\n",
    "                try:\n",
    "                    items = [int(i) for i in l[1:]]\n",
    "                except:\n",
    "                    items = []\n",
    "                uid = int(l[0])\n",
    "                testUniqueUsers.append(uid)\n",
    "                testUser.extend([uid] * len(items))\n",
    "                testItem.extend(items)\n",
    "                try:\n",
    "                    m_item = max(m_item, max(items))\n",
    "                except:\n",
    "                    m_item = m_item\n",
    "                n_user = max(n_user, uid)\n",
    "                testDataSize += len(items)\n",
    "\n",
    "    train_data = []\n",
    "    test_data = []\n",
    "\n",
    "    n_user += 1\n",
    "    m_item += 1\n",
    "\n",
    "    for i in range(len(trainUser)):\n",
    "        train_data.append([trainUser[i], trainItem[i]])\n",
    "    for i in range(len(testUser)):\n",
    "        test_data.append([testUser[i], testItem[i]])\n",
    "    train_mat = sp.dok_matrix((n_user, m_item), dtype=np.float32)\n",
    "\n",
    "    for x in train_data:\n",
    "        train_mat[x[0], x[1]] = 1.0\n",
    "\n",
    "    # construct degree matrix for graphmf\n",
    "\n",
    "    items_D = np.sum(train_mat, axis = 0).reshape(-1)\n",
    "    users_D = np.sum(train_mat, axis = 1).reshape(-1)\n",
    "\n",
    "    beta_uD = (np.sqrt(users_D + 1) / users_D).reshape(-1, 1)\n",
    "    beta_iD = (1 / np.sqrt(items_D + 1)).reshape(1, -1)\n",
    "\n",
    "    constraint_mat = {\"beta_uD\": torch.from_numpy(beta_uD).reshape(-1),\n",
    "                      \"beta_iD\": torch.from_numpy(beta_iD).reshape(-1)}\n",
    "\n",
    "    return train_data, test_data, train_mat, n_user, m_item, constraint_mat\n",
    "\n",
    "\n",
    "\n",
    "def data_param_prepare(config_file):\n",
    "\n",
    "    config = configparser.ConfigParser()\n",
    "    config.read(config_file)\n",
    "\n",
    "    params = {}\n",
    "\n",
    "    params['embedding_dim'] = config.getint('Model', 'embedding_dim')\n",
    "    ii_neighbor_num = config.getint('Model', 'ii_neighbor_num')\n",
    "    params['ii_neighbor_num'] = ii_neighbor_num\n",
    "    model_save_path = config['Model']['model_save_path']\n",
    "    params['model_save_path'] = model_save_path\n",
    "    params['max_epoch'] = config.getint('Model', 'max_epoch')\n",
    "\n",
    "    params['enable_tensorboard'] = config.getboolean('Model', 'enable_tensorboard')\n",
    "    \n",
    "    initial_weight = config.getfloat('Model', 'initial_weight')\n",
    "    params['initial_weight'] = initial_weight\n",
    "\n",
    "    dataset = config['Training']['dataset']\n",
    "    params['dataset'] = dataset\n",
    "    train_file_path = config['Training']['train_file_path']\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else \"cpu\")\n",
    "    params['device'] = device\n",
    "    lr = config.getfloat('Training', 'learning_rate')\n",
    "    params['lr'] = lr\n",
    "    batch_size = config.getint('Training', 'batch_size')\n",
    "    params['batch_size'] = batch_size\n",
    "    early_stop_epoch = config.getint('Training', 'early_stop_epoch')\n",
    "    params['early_stop_epoch'] = early_stop_epoch\n",
    "    w1 = config.getfloat('Training', 'w1')\n",
    "    w2 = config.getfloat('Training', 'w2')\n",
    "    w3 = config.getfloat('Training', 'w3')\n",
    "    w4 = config.getfloat('Training', 'w4')\n",
    "    params['w1'] = w1\n",
    "    params['w2'] = w2\n",
    "    params['w3'] = w3\n",
    "    params['w4'] = w4\n",
    "    negative_num = config.getint('Training', 'negative_num')\n",
    "    negative_weight = config.getfloat('Training', 'negative_weight')\n",
    "    params['negative_num'] = negative_num\n",
    "    params['negative_weight'] = negative_weight\n",
    "\n",
    "    gamma = config.getfloat('Training', 'gamma')\n",
    "    params['gamma'] = gamma\n",
    "    lambda_ = config.getfloat('Training', 'lambda')\n",
    "    params['lambda'] = lambda_\n",
    "    sampling_sift_pos = config.getboolean('Training', 'sampling_sift_pos')\n",
    "    params['sampling_sift_pos'] = sampling_sift_pos\n",
    "    \n",
    "    test_batch_size = config.getint('Testing', 'test_batch_size')\n",
    "    params['test_batch_size'] = test_batch_size\n",
    "    topk = config.getint('Testing', 'topk') \n",
    "    params['topk'] = topk\n",
    "\n",
    "    test_file_path = config['Testing']['test_file_path']\n",
    "\n",
    "    # dataset processing\n",
    "    train_data, test_data, train_mat, user_num, item_num, constraint_mat = load_data(train_file_path, test_file_path)\n",
    "    train_loader = data.DataLoader(train_data, batch_size=batch_size, shuffle = True, num_workers=5)\n",
    "    test_loader = data.DataLoader(list(range(user_num)), batch_size=test_batch_size, shuffle=False, num_workers=5)\n",
    "\n",
    "    params['user_num'] = user_num\n",
    "    params['item_num'] = item_num\n",
    "\n",
    "    # mask matrix for testing to accelarate testing speed\n",
    "    mask = torch.zeros(user_num, item_num)\n",
    "    interacted_items = [[] for _ in range(user_num)]\n",
    "    for (u, i) in train_data:\n",
    "        mask[u][i] = -np.inf\n",
    "        interacted_items[u].append(i)\n",
    "\n",
    "    # test user-item interaction, which is ground truth\n",
    "    test_ground_truth_list = [[] for _ in range(user_num)]\n",
    "    for (u, i) in test_data:\n",
    "        test_ground_truth_list[u].append(i)\n",
    "\n",
    "    \n",
    "    ii_neighbor_mat, ii_constraint_mat = get_ii_constraint_mat(train_mat, ii_neighbor_num)\n",
    "\n",
    "    return params, constraint_mat, ii_constraint_mat, ii_neighbor_mat, train_loader, test_loader, mask, test_ground_truth_list, interacted_items\n",
    "\n",
    "def get_ii_constraint_mat(train_mat, num_neighbors, ii_diagonal_zero = False):\n",
    "    print('Computing \\\\Omega for the item-item graph... ')\n",
    "    A = train_mat.T.dot(train_mat)\t# I * I\n",
    "    n_items = A.shape[0]\n",
    "    res_mat = torch.zeros((n_items, num_neighbors))\n",
    "    res_sim_mat = torch.zeros((n_items, num_neighbors))\n",
    "    if ii_diagonal_zero:\n",
    "        A[range(n_items), range(n_items)] = 0\n",
    "    items_D = np.sum(A, axis = 0).reshape(-1)\n",
    "    users_D = np.sum(A, axis = 1).reshape(-1)\n",
    "\n",
    "    beta_uD = (np.sqrt(users_D + 1) / users_D).reshape(-1, 1)\n",
    "    beta_iD = (1 / np.sqrt(items_D + 1)).reshape(1, -1)\n",
    "    all_ii_constraint_mat = torch.from_numpy(beta_uD.dot(beta_iD))\n",
    "    for i in range(n_items):\n",
    "        row = all_ii_constraint_mat[i] * torch.from_numpy(A.getrow(i).toarray()[0])\n",
    "        row_sims, row_idxs = torch.topk(row, num_neighbors)\n",
    "        res_mat[i] = row_idxs\n",
    "        res_sim_mat[i] = row_sims\n",
    "        if i % 15000 == 0:\n",
    "            print('i-i constraint matrix {} ok'.format(i))\n",
    "\n",
    "    print('Computation \\\\Omega OK!')\n",
    "    return res_mat.long(), res_sim_mat.float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UltraGCN(nn.Module):\n",
    "    def __init__(self, params, constraint_mat, ii_constraint_mat, ii_neighbor_mat):\n",
    "        super(UltraGCN, self).__init__()\n",
    "        self.user_num = params['user_num']\n",
    "        self.item_num = params['item_num']\n",
    "        self.embedding_dim = params['embedding_dim']\n",
    "        self.w1 = params['w1']\n",
    "        self.w2 = params['w2']\n",
    "        self.w3 = params['w3']\n",
    "        self.w4 = params['w4']\n",
    "\n",
    "        self.negative_weight = params['negative_weight']\n",
    "        self.gamma = params['gamma']\n",
    "        self.lambda_ = params['lambda']\n",
    "\n",
    "        self.user_embeds = nn.Embedding(self.user_num, self.embedding_dim)\n",
    "        self.item_embeds = nn.Embedding(self.item_num, self.embedding_dim)\n",
    "\n",
    "        self.constraint_mat = constraint_mat\n",
    "\n",
    "\n",
    "\n",
    "        self.constraint_mat['beta_uD'] = self.constraint_mat['beta_uD'].to(params['device'])\n",
    "        self.constraint_mat['beta_iD'] = self.constraint_mat['beta_iD'].to(params['device'])\n",
    "\n",
    "        self.ii_constraint_mat = ii_constraint_mat\n",
    "        self.ii_constraint_mat = self.ii_constraint_mat.to( params['device'])\n",
    "        \n",
    "        self.ii_neighbor_mat = ii_neighbor_mat\n",
    "        self.ii_neighbor_mat = self.ii_neighbor_mat.to( params['device'])\n",
    "\n",
    "        self.initial_weight = params['initial_weight']\n",
    "        self.initial_weights()\n",
    "\n",
    "    def initial_weights(self):\n",
    "        nn.init.normal_(self.user_embeds.weight, std=self.initial_weight)\n",
    "        nn.init.normal_(self.item_embeds.weight, std=self.initial_weight)\n",
    "\n",
    "    def get_omegas(self, users, pos_items, neg_items):\n",
    "        device = self.get_device()\n",
    "        if self.w2 > 0:\n",
    "            pos_weight = torch.mul(self.constraint_mat['beta_uD'][users], self.constraint_mat['beta_iD'][pos_items]).to(device)\n",
    "            pos_weight = self.w1 + self.w2 * pos_weight\n",
    "        else:\n",
    "            pos_weight = self.w1 * torch.ones(len(pos_items)).to(device)\n",
    "        \n",
    "        # users = (users * self.item_num).unsqueeze(0)\n",
    "        if self.w4 > 0:\n",
    "            neg_weight = torch.mul(torch.repeat_interleave(self.constraint_mat['beta_uD'][users], neg_items.size(1)), self.constraint_mat['beta_iD'][neg_items.flatten()]).to(device)\n",
    "            neg_weight = self.w3 + self.w4 * neg_weight\n",
    "        else:\n",
    "            neg_weight = self.w3 * torch.ones(neg_items.size(0) * neg_items.size(1)).to(device)\n",
    "\n",
    "\n",
    "        weight = torch.cat((pos_weight, neg_weight))\n",
    "        return weight\n",
    "\n",
    "    def cal_loss_L(self, users, pos_items, neg_items, omega_weight):\n",
    "        device = self.get_device()\n",
    "        user_embeds = self.user_embeds(users)\n",
    "        pos_embeds = self.item_embeds(pos_items)\n",
    "        neg_embeds = self.item_embeds(neg_items)\n",
    "      \n",
    "        pos_scores = (user_embeds * pos_embeds).sum(dim=-1) # batch_size\n",
    "        user_embeds = user_embeds.unsqueeze(1)\n",
    "        neg_scores = (user_embeds * neg_embeds).sum(dim=-1) # batch_size * negative_num\n",
    "\n",
    "        neg_labels = torch.zeros(neg_scores.size()).to(device)\n",
    "        neg_loss = F.binary_cross_entropy_with_logits(neg_scores, neg_labels, weight = omega_weight[len(pos_scores):].view(neg_scores.size()), reduction='none').mean(dim = -1)\n",
    "        \n",
    "        pos_labels = torch.ones(pos_scores.size()).to(device)\n",
    "        pos_loss = F.binary_cross_entropy_with_logits(pos_scores, pos_labels, weight = omega_weight[:len(pos_scores)], reduction='none')\n",
    "\n",
    "        loss = pos_loss + neg_loss * self.negative_weight\n",
    "      \n",
    "        return loss.sum()\n",
    "\n",
    "    def cal_loss_I(self, users, pos_items):\n",
    "        device = self.get_device()\n",
    "        neighbor_embeds = self.item_embeds(self.ii_neighbor_mat[pos_items].to(device))    # len(pos_items) * num_neighbors * dim\n",
    "        sim_scores = self.ii_constraint_mat[pos_items].to(device)     # len(pos_items) * num_neighbors\n",
    "        user_embeds = self.user_embeds(users).unsqueeze(1)\n",
    "        \n",
    "        loss = -sim_scores * (user_embeds * neighbor_embeds).sum(dim=-1).sigmoid().log()\n",
    "      \n",
    "        # loss = loss.sum(-1)\n",
    "        return loss.sum()\n",
    "\n",
    "    def norm_loss(self):\n",
    "        loss = 0.0\n",
    "        for parameter in self.parameters():\n",
    "            loss += torch.sum(parameter ** 2)\n",
    "        return loss / 2\n",
    "\n",
    "    def forward(self, users, pos_items, neg_items):\n",
    "        omega_weight = self.get_omegas(users, pos_items, neg_items)\n",
    "        \n",
    "        loss = self.cal_loss_L(users, pos_items, neg_items, omega_weight)\n",
    "        loss += self.gamma * self.norm_loss()\n",
    "        loss += self.lambda_ * self.cal_loss_I(users, pos_items)\n",
    "        return loss\n",
    "\n",
    "    def test_foward(self, users):\n",
    "        items = torch.arange(self.item_num).to(users.device)\n",
    "        user_embeds = self.user_embeds(users)\n",
    "        item_embeds = self.item_embeds(items)\n",
    "         \n",
    "        return user_embeds.mm(item_embeds.t())\n",
    "\n",
    "    def get_device(self):\n",
    "        return self.user_embeds.weight.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Sampling(pos_train_data, item_num, neg_ratio, interacted_items, sampling_sift_pos):\n",
    "\tneg_candidates = np.arange(item_num)\n",
    "\n",
    "\tif sampling_sift_pos:\n",
    "\t\tneg_items = []\n",
    "\t\tfor u in pos_train_data[0]:\n",
    "\t\t\tprobs = np.ones(item_num)\n",
    "\t\t\tprobs[interacted_items[u]] = 0\n",
    "\t\t\tprobs /= np.sum(probs)\n",
    "\n",
    "\t\t\tu_neg_items = np.random.choice(neg_candidates, size = neg_ratio, p = probs, replace = True).reshape(1, -1)\n",
    "\t\n",
    "\t\t\tneg_items.append(u_neg_items)\n",
    "\n",
    "\t\tneg_items = np.concatenate(neg_items, axis = 0) \n",
    "\telse:\n",
    "\t\tneg_items = np.random.choice(neg_candidates, (len(pos_train_data[0]), neg_ratio), replace = True)\n",
    "\t\n",
    "\tneg_items = torch.from_numpy(neg_items)\n",
    "\t\n",
    "\treturn pos_train_data[0], pos_train_data[1], neg_items\t# users, pos_items, neg_items\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# TRAINING #\n",
    "\n",
    "def train(model, optimizer, train_loader, test_loader, mask, test_ground_truth_list, interacted_items, params): \n",
    "    device = params['device']\n",
    "    best_epoch, best_recall, best_ndcg = 0, 0, 0\n",
    "    early_stop_count = 0\n",
    "    early_stop = False\n",
    "\n",
    "    batches = len(train_loader.dataset) // params['batch_size']\n",
    "    if len(train_loader.dataset) % params['batch_size'] != 0:\n",
    "        batches += 1\n",
    "    print('Total training batches = {}'.format(batches))\n",
    "    \n",
    "    if params['enable_tensorboard']:\n",
    "        writer = SummaryWriter()\n",
    "\n",
    "    for epoch in range(params['max_epoch']):\n",
    "        model.train() \n",
    "        start_time = time.time()\n",
    "\n",
    "        for batch, x in enumerate(train_loader): # x: tensor:[users, pos_items]\n",
    "            users, pos_items, neg_items = Sampling(x, params['item_num'], params['negative_num'], interacted_items, params['sampling_sift_pos'])\n",
    "            users = users.to(device)\n",
    "            pos_items = pos_items.to(device)\n",
    "            neg_items = neg_items.to(device)\n",
    "\n",
    "            model.zero_grad()\n",
    "            loss = model(users, pos_items, neg_items)\n",
    "            if params['enable_tensorboard']:\n",
    "                writer.add_scalar(\"Loss/train_batch\", loss, batches * epoch + batch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        \n",
    "        train_time = time.strftime(\"%H: %M: %S\", time.gmtime(time.time() - start_time))\n",
    "        if params['enable_tensorboard']:\n",
    "            writer.add_scalar(\"Loss/train_epoch\", loss, epoch)\n",
    "\n",
    "        need_test = True\n",
    "        if epoch < 50 and epoch % 5 != 0:\n",
    "            need_test = False\n",
    "            \n",
    "        if need_test:\n",
    "            start_time = time.time()\n",
    "            F1_score, Precision, Recall, NDCG = test(model, test_loader, test_ground_truth_list, mask, params['topk'], params['user_num'])\n",
    "            if params['enable_tensorboard']:\n",
    "                writer.add_scalar('Results/recall@20', Recall, epoch)\n",
    "                writer.add_scalar('Results/ndcg@20', NDCG, epoch)\n",
    "            test_time = time.strftime(\"%H: %M: %S\", time.gmtime(time.time() - start_time))\n",
    "            \n",
    "            print('The time for epoch {} is: train time = {}, test time = {}'.format(epoch, train_time, test_time))\n",
    "            print(\"Loss = {:.5f}, F1-score: {:5f} \\t Precision: {:.5f}\\t Recall: {:.5f}\\tNDCG: {:.5f}\".format(loss.item(), F1_score, Precision, Recall, NDCG))\n",
    "\n",
    "            if Recall > best_recall:\n",
    "                best_recall, best_ndcg, best_epoch = Recall, NDCG, epoch\n",
    "                early_stop_count = 0\n",
    "                torch.save(model.state_dict(), params['model_save_path'])\n",
    "\n",
    "            else:\n",
    "                early_stop_count += 1\n",
    "                if early_stop_count == params['early_stop_epoch']:\n",
    "                    early_stop = True\n",
    "        \n",
    "        if early_stop:\n",
    "            print('###')\n",
    "            print('Early stop is triggered at {} epochs.'.format(epoch))\n",
    "            print('Results:')\n",
    "            print('best epoch = {}, best recall = {}, best ndcg = {}'.format(best_epoch, best_recall, best_ndcg))\n",
    "            print('The best model is saved at {}'.format(params['model_save_path']))\n",
    "            break\n",
    "\n",
    "    writer.flush()\n",
    "\n",
    "    print('Training end!')\n",
    "\n",
    "# TESTING #\n",
    "\n",
    "def hit(gt_item, pred_items):\n",
    "\tif gt_item in pred_items:\n",
    "\t\treturn 1\n",
    "\treturn 0\n",
    "\n",
    "\n",
    "def ndcg(gt_item, pred_items):\n",
    "\tif gt_item in pred_items:\n",
    "\t\tindex = pred_items.index(gt_item)\n",
    "\t\treturn np.reciprocal(np.log2(index+2))\n",
    "\treturn 0\n",
    "\n",
    "\n",
    "def RecallPrecision_ATk(test_data, r, k):\n",
    "\t\"\"\"\n",
    "    test_data : list\n",
    "    pred_data : shape (test_batch, k) NOTE: pred_data should be pre-sorted\n",
    "    k : top-k\n",
    "    \"\"\"\n",
    "\tright_pred = r[:, :k].sum(1)\n",
    "\tprecis_n = k\n",
    "\t\n",
    "\trecall_n = np.array([len(test_data[i]) for i in range(len(test_data))])\n",
    "\trecall_n = np.where(recall_n != 0, recall_n, 1)\n",
    "\trecall = np.sum(right_pred / recall_n)\n",
    "\tprecis = np.sum(right_pred) / precis_n\n",
    "\treturn {'recall': recall, 'precision': precis}\n",
    "\n",
    "\n",
    "def MRRatK_r(r, k):\n",
    "\t\"\"\"\n",
    "    Mean Reciprocal Rank\n",
    "    \"\"\"\n",
    "\tpred_data = r[:, :k]\n",
    "\tscores = np.log2(1. / np.arange(1, k + 1))\n",
    "\tpred_data = pred_data / scores\n",
    "\tpred_data = pred_data.sum(1)\n",
    "\treturn np.sum(pred_data)\n",
    "\n",
    "\n",
    "def NDCGatK_r(test_data, r, k):\n",
    "\t\"\"\"\n",
    "    Normalized Discounted Cumulative Gain\n",
    "    rel_i = 1 or 0, so 2^{rel_i} - 1 = 1 or 0\n",
    "    \"\"\"\n",
    "\tassert len(r) == len(test_data)\n",
    "\tpred_data = r[:, :k]\n",
    "\n",
    "\ttest_matrix = np.zeros((len(pred_data), k))\n",
    "\tfor i, items in enumerate(test_data):\n",
    "\t\tlength = k if k <= len(items) else len(items)\n",
    "\t\ttest_matrix[i, :length] = 1\n",
    "\tmax_r = test_matrix\n",
    "\tidcg = np.sum(max_r * 1. / np.log2(np.arange(2, k + 2)), axis=1)\n",
    "\tdcg = pred_data * (1. / np.log2(np.arange(2, k + 2)))\n",
    "\tdcg = np.sum(dcg, axis=1)\n",
    "\tidcg[idcg == 0.] = 1.\n",
    "\tndcg = dcg / idcg\n",
    "\tndcg[np.isnan(ndcg)] = 0.\n",
    "\treturn np.sum(ndcg)\n",
    "\n",
    "\n",
    "def test_one_batch(X, k):\n",
    "    sorted_items = X[0].numpy()\n",
    "    groundTrue = X[1]\n",
    "    r = getLabel(groundTrue, sorted_items)\n",
    "    ret = RecallPrecision_ATk(groundTrue, r, k)\n",
    "    return ret['precision'], ret['recall'], NDCGatK_r(groundTrue,r,k)\n",
    "\n",
    "def getLabel(test_data, pred_data):\n",
    "    r = []\n",
    "    for i in range(len(test_data)):\n",
    "        groundTrue = test_data[i]\n",
    "        predictTopK = pred_data[i]\n",
    "        pred = list(map(lambda x: x in groundTrue, predictTopK))\n",
    "        pred = np.array(pred).astype(\"float\")\n",
    "        r.append(pred)\n",
    "    return np.array(r).astype('float')\n",
    "\n",
    "\n",
    "def test(model, test_loader, test_ground_truth_list, mask, topk, n_user):\n",
    "    rating_list = []\n",
    "    groundTrue_list = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        for idx, batch_users in enumerate(test_loader):\n",
    "            \n",
    "            batch_users = batch_users.to(model.get_device())\n",
    "            rating = model.test_foward(batch_users) \n",
    "            rating = rating.cpu()\n",
    "            batch_users = batch_users.cpu()\n",
    "            rating += mask[batch_users]\n",
    "            \n",
    "            _, rating_K = torch.topk(rating, k=topk)\n",
    "            rating_list.append(rating_K)\n",
    "\n",
    "            groundTrue_list.append([test_ground_truth_list[u] for u in batch_users])\n",
    "\n",
    "    X = zip(rating_list, groundTrue_list)\n",
    "    Recall, Precision, NDCG = 0, 0, 0\n",
    "\n",
    "    for i, x in enumerate(X):\n",
    "        precision, recall, ndcg = test_one_batch(x, topk)\n",
    "        Recall += recall\n",
    "        Precision += precision\n",
    "        NDCG += ndcg\n",
    "        \n",
    "    Precision /= n_user\n",
    "    Recall /= n_user\n",
    "    NDCG /= n_user\n",
    "    F1_score = 2 * (Precision * Recall) / (Precision + Recall)\n",
    "\n",
    "    return F1_score, Precision, Recall, NDCG\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initiate(args):\n",
    "\n",
    "\n",
    "    print('Loading Configuration...')\n",
    "    params, constraint_mat, ii_constraint_mat, ii_neighbor_mat, train_loader, test_loader, mask, test_ground_truth_list, interacted_items = data_param_prepare(args.config_file)\n",
    "\n",
    "    print('Load Configuration OK, show them below')\n",
    "    print('Configuration:')\n",
    "    print(params)\n",
    "\n",
    "    ultragcn = UltraGCN(params, constraint_mat, ii_constraint_mat, ii_neighbor_mat)\n",
    "    ultragcn = ultragcn.to(params['device'])\n",
    "    optimizer = torch.optim.Adam(ultragcn.parameters(), lr=params['lr'])\n",
    "\n",
    "    train(ultragcn, optimizer, train_loader, test_loader, mask, test_ground_truth_list, interacted_items, params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "initiate({\"config_file\": \"./ultragcn_yelp18_m1.ini\" })\n",
    "# initiate({\"config_file\": \"./ultragcn_gowalla_m1.ini\" })\n",
    "# initiate({\"config_file\": \"./ultragcn_amazonbooks_m1.ini\" })\n",
    "# initiate({\"config_file\": \"./ultragcn_movielens1m_m1.ini\" })\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
